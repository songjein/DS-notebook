{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트와 시퀀스를 처리하는 기본적인 모델\n",
    "- Recurrent Neural network\n",
    "- 1D convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 애플리케이션\n",
    "- 문서, 시계열 분류 (글의 주제 혹은 저자 식별)\n",
    "- 시계열 비교, 두 시퀀스가 얼마나 밀접한 관련이 있는지\n",
    "- seq-to-seq 번역\n",
    "- 감성 분석 (긍/부정)\n",
    "- 시계열 예측, 어떤 지여그이 최근 날씨 데이터 -> 향후 날씨 예측\n",
    "- 질의 응답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 데이터 다루기\n",
    "- 단어의 시퀀스 혹은 문자의 시퀀스\n",
    "- 문자 언어에 대한 통계적 구조를 만들어 간단한 문제 해결\n",
    "- 자연어 처리를 위한 딥러닝은 단어, 문장, 문단에 대한 **패턴 인식**\n",
    "- 텍스트를 처리하기 위해 **텍스트 벡터화(vectorizing text)** 해야함\n",
    "    - 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환\n",
    "    - 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환\n",
    "    - 텍스트에서 단어나 문자의 n-gram을 추출하여 각 n-gram을 하나의 벡터로 변환\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 벡터화\n",
    "1. 토큰화(tokenization) \n",
    "2. 토큰에 수치형 벡터 연결\n",
    "    - one-hot encoding\n",
    "    - token embedding(word embedding; 일반적으로 단어에 적용하기 때문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram, BoW\n",
    "- word n-gram\n",
    "    - { The, The cat, cat, cat sat, sat, sat on, on, on the, ... } bag of 2-gram\n",
    "- Bag-of-Words ; 특정한 순서 x, 집합\n",
    "- But, 얕은 학습법에 적합한 방법임, 순서 정보가 다 사라져버림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이후의 과정은 \n",
    "- [원 핫 인코딩](./deep-learning-with-python/6.1-one-hot-encoding-of-words-or-characters.ipynb)\n",
    "- [워드 임베딩](./deep-learning-with-python/6.1-using-word-embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지 살펴본 뉴럴넷에선 메모리가 없음\n",
    "- 입력은 개별적으로 처리되고, 입력 간에 유지되는 상태란 없음\n",
    "    - 이런 네트워크는 feedforward network라고 함\n",
    "- 이와 달리, 이전에 나온 것을 기억하면서 처리할 수 가 있음\n",
    "    - 이를 Recurrent Neural Network로 표현할 수 있음, 순환신경망\n",
    "    - 시퀀스의 원소를 순회하면서 지금까지 처리한 정보를 상태(state)에 저장\n",
    "        - 이 상태는 서로 다른 시퀀스를 처리 할 때는 재설정 됨\n",
    "        - 하나의 시퀀스가 여전히 하나의 데이터 포인트로 간주됨\n",
    "        - 대신 데이터 포인트가 한 번에 처리되는게 아니라 원소를 차례대로 처리함\n",
    "    - RNN은 (timesteps, input_features) 모양의 2D 텐서로 인코딩된 벡터의 시퀀스를 입력받음\n",
    "    - 이 시퀀스는 타임스텝을 따라 반복되며,\n",
    "    - 각 타임스텝 t에서 **이전 상태와 현재 입력을 둘 다 고려해서 출력을 계산**\n",
    "    - 그 다음, 이 출력을 다음 스텝의 상태로 설정\n",
    "    - 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 정방향 계산 예시\n",
    "- 이전 타임스텝의 출력을 은닉 상태라 부름(**Hidden State**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 100\n",
    "input_features = 32\n",
    "output_features = 64\n",
    "\n",
    "inputs = np.random.random((timesteps, input_features))\n",
    "\n",
    "state_t = np.zeros((output_features,))\n",
    "\n",
    "# weights matrix\n",
    "W = np.random.random((output_features, input_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features,))\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:    # input_t'shape == (input_features, )\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "    successive_outputs.append(output_t)\n",
    "    state_t = output_t\n",
    "\n",
    "# 최종 출력의 모양은 (timesteps, output_features)\n",
    "final_output_sequence = np.stack(successive_outputs, axis=0)\n",
    "final_output_sequence[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이후의 과정은\n",
    "- [RNN](./deep-learning-with-python/6.2-understanding-recurrent-neural-networks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM, GRU 이해\n",
    "- SimpleRNN은 이론적으로 시간 t에서 이전의 모든 타임스텝의 정보를 유지할 수 있지만\n",
    "    - 실제로 긴 시간에 걸친 의존성은 학습할 수 없다\n",
    "    - Vanishing Gradient Problem\n",
    "        - RNN의 학습은 timesteps에 따라 네트워크를 펼쳐놓은 것 처럼 진행\n",
    "        - BackPropagation Through Time\n",
    "        - 구조가 재귀적이기 때문에, 타임스텝이 길어질 수록 그래디언트 값이 vanishing 혹은 exploding함\n",
    "- Long Short Term Memory\n",
    "    - RNN의 변종으로, 정보를 여러 타임스텝에 걸쳐 나르는 방법이 추가된다.\n",
    "    - 처리되는 시퀀스와 나란히 컨베이어 벨트가 있다고 생각 (**Cell State**)\n",
    "        - 시퀀스를 처리하다가 추출된 중요한 정보가 컨베이어로 올라가 필요한 시점에 끄집어 내 씀\n",
    "        - **forget gate**가 정보를 적절히 망각을하고\n",
    "        - **cell state**에 추가할 정보를 생성 및 input gate를 통해 불순물을 제거하고\n",
    "        - forget gate에서 설정된 비율을 기존의 cell state에 곱하고 불순물이 제거된 새로운 정보가 합해져 **cell state가 업데이트**되고\n",
    "        - **output gate**에 의해 설정된 비율을 통해 cell state의 정보를 필터링해서 아웃풋을 생성\n",
    "            - 라고 하지만, 실제로 하는일은 셀의 가중치에 의해 결정됨, 가설일 뿐\n",
    "    - 중요한건, 중요한 과거 정보를 유지하는 기능이 있다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이후의 과정은\n",
    "- [LSTM#케라스를 사용한 LSTM 예제부분](./deep-learning-with-python/6.2-understanding-recurrent-neural-networks.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
