{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "코드는 github에 다 공개되어있음  \n",
    "\n",
    "## 표현을 학습하다\n",
    "- 알고 있는 입력, 출력의 샘플로 부터 학습  \n",
    "- 입력 데이터를 기반으로 기대 출력에 가깝게 만드는 유용한 **Representation**을 학습  \n",
    "\n",
    "## Representation? \n",
    "- 데이터를 Encoding 하거나 묘사하기 위해 데이터를 바라보는 방법\n",
    "    - ex) 컬러 이미지는 RGB 포맷이나 HSV 포맷으로 인코딩 될 수 있음\n",
    "    - 어떤 표현으로 해결하기 힘든 문제가, 다른 표현으로는 쉽게 해결될 수도\n",
    "        - 빨간색 픽셀을 택하는 문제는 RGB 포맷\n",
    "        - 채도를 낮추는 문제는 HSV포맷\n",
    "- 머신러닝 모델은 입력 데이터에서 **적절한 표현(Representation)**을 찾는 것\n",
    "    - +ex) 2차원 좌표 데이터를 회전 시켜서 분류하는데 적합한 새로운 좌표를 부여하는 것도 이에 해당\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep\n",
    "- 레이어를 거침에 따라 원본 입력(이미지)와는 점점 다른 표현으로 변환이 됨\n",
    "- 연속된 필터를 통과한다고 생각하면 되고, 점점 정보들이 정제되는 과정이라 생각할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "- 주어진 입력을 정확한 타겟으로 매핑하기 위해\n",
    "- 각 층에 있는 가중치 값을 찾는 것!\n",
    "- 찾는 다는 것은 **가중치를 조정** 한다는 것\n",
    "    - 이를 위해선 관찰이 필요한데, 이를 위해 **Loss function** 혹은 **Objective function**을 정의\n",
    "    - 이를 이용해 우리의 모델(신경망)이 예측한 값과 진짜 타겟 값의 차이를 계산 할 수 있음\n",
    "    - 이후에 **Backpropagation**을 이용해서 가중치 업데이트(이를 구현한 것이 optimizer들)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 충분치 않을 때는 기존의 머신러닝 방법이 더 적합할 수 있으니    \n",
    "딥러닝이라는 해머를 들고 모든 문제를 못처럼 바라보지 마라"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝의 흐름 간략히\n",
    "1. Probabilistic modeling(Bayes' theorem), Logistic Regression(Classification)\n",
    "2. Kernel method\n",
    "    - **SVM** ; 2개의 다른 범주에 속한 데이터 그룹 사이에 좋은 결정경계(**decision boundary**) 찾기\n",
    "        1. 결정경계가 하나의 초평면(hyperplane)으로 표현될 수 있는 새로운 고차원 표현으로 데이터를 매핑\n",
    "        2. 초평면과 각 클래스의 가장 가까운 데이터 포인트 사이의 거리가 최대가 되는 최선의 결정경계 찾기  \n",
    "        'maximizing the margin'\n",
    "    - 분류 문제를 간단히 만들기 위해 데이터를 고차원 표현으로 매핑하는 것은 좋아보이지만 구현이 어렵스\n",
    "        - kernel trick을 쓰자\n",
    "            - 새롭게 표현된 공간에서 좋은 결정 초평면을 찾기 위해 새로운 공간에 대응하는 데이터포인트의 실제 좌표를 구할 필요가 없음\n",
    "            - 새로운 공간에서의 두 데이터 포인트 사이의 거리만 계산할 수 있으면 된다.\n",
    "            - kernel function을 쓰면 이를 효율적으로 계산\n",
    "                - 원본 공간에 있는 두 데이터 포인트를 명시적으로 새로운 표현으로 변환하지 않고 타깃 표현 공간에서 위치했을 때 거리를 매핑해주는 연산\n",
    "                - but, 이 함수는 학습되는게 아닌 직접 만드는 것, 오직 분할 초평면만 학습됨\n",
    "        - 대용량의 데이터에 대해 scalable하지 못함, feature engineering 작업이 중요한데 수동으로 해야함\n",
    "3. Decision Tree, Random Forest, **Gradient Boosting Machine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "- Neural Net에 알 맞는 Activation Function\n",
    "- Weight Initialization\n",
    "- Optimization method\n",
    "- etc ; 기울기를 더 잘 전파할 수 있는 \n",
    "    - Batch norm \n",
    "    - Residual connection\n",
    "    - Depthwise Separable Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝이 피쳐 엔지니어링을 필요 없게 만든다면, kaggle에서 문제 풀 때도 deep learning 방법으로 피쳐를 먼저 뽑고  \n",
    "해당 피쳐 대상으로 Gradient Boosting을 적용해볼까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어들\n",
    "- Training / Test set\n",
    "- Representation ; 일종의 데이터 처리 필터라 할 수 있는 Layer를 거쳐 추출되는 것\n",
    "- Softmax Layer; 쉽게 말해 n개의 확률 점수가 들어 있는 배열을 반환하는 레이어\n",
    "- Loss function ; 성능 측정\n",
    "- Optimizer ; 네트워크 업데이트 알고리즘\n",
    "- Tensor\n",
    "    - 0D : scalar, ()\n",
    "    - 1D : vector, (n,)\n",
    "    - 2D : matrix, (m,n)\n",
    "    - 축의 개수(랭크) ; .ndim\n",
    "    - 텐서 조작\n",
    "        - train_images[:, 7:-7, 7:-7] 정중앙 14x14 px만 자르기\n",
    "- Batch\n",
    "    - 보통 0번째 축이 sample axis\n",
    "        - vector data: (samples, features)\n",
    "        - sequence data: (samples, timesteps, features)\n",
    "        - image data: (samples, height, width, channels) or (samples, channels, height, width)\n",
    "            - 참고로 tensorflow 방식이 channel last 방식\n",
    "        - video data: (samples, frames, height, width, chennels) or (samples, frames, channels, height, width)\n",
    "    - 한번에 전체 데이터 처리 하지 않고 작은 batch로 나눔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 연산\n",
    "- element-wise operation ; 텐서에 속한 원소들에게 독립적으로 적용되는 연산\n",
    "    - relu(dot(W, input) + b)\n",
    "- broadcasting\n",
    "    - 크기가 다른 두 텐서가 더해질 때\n",
    "        1. 큰 텐서의 ndim에 맞도록 작은 텐서에 브로드캐스팅 축 추가\n",
    "        2. 작은 텐서가 새 축을 따라 큰 텐서의 크기에 맞도록 반복됨\n",
    "            - X: (32, 10), y: (10,)\n",
    "            - y -> (1, 10) 축 추가하고, 32번 반복하면 (32, 10)이 됨\n",
    "    - ex)\n",
    "    ```py\n",
    "    x = np.random.random((64,3,32,10))\n",
    "    y = np.random.random((32,10))\n",
    "    \n",
    "    z = np.maximum(x, y)\n",
    "    ```\n",
    "- tensor product; dot operation\n",
    "    - 벡터 끼리 dot을 수행 할 땐 내적이고\n",
    "    - 벡터 x 매트릭스 끼리 할 때는 내적이긴 한데, 행렬 곱의 모양을 따르는 내적\n",
    "    - (a, b, c, d) . (d,) -> (a, b, c)\n",
    "    - (a, b, c, d) . (d, e) -> (a, b, c, e)\n",
    "- 텐서 연산들이 조작하는 텐서의 내용은 기하학적 공간에 있는 좌표 포인트로 해석될 수 있음\n",
    "    - 텐서에 어떤 매트릭스가 곱해지냐에 따라, 회전이 될 수도있고, 스케일링이 될 수도 있음\n",
    "    - 이런 변환을 통해 적절한 데이터 표현을 찾아 나가는 것이라 할 수 있을 듯\n",
    "    - 레이어를 거침에 따라 복합하게 꼬여있던 입력 데이터의 표현이 문제를 풀기 수월한 표현으로 조금씩 바뀜\n",
    "        - 빨강/파랑 색종이 비유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient 기반 Optimization\n",
    "- 신경망에 사용된 연산이 **미분 가능(differentiable)** 하다면, Weight에 대한 Loss의 Gradient를 계산해서 \n",
    "    - weight을 gredient의 반대 방향으로 이동시킴으로써 Loss를 감소 시키게끔 업데이트 할 수 있다.\n",
    "- 입력 벡터 x, 가중치 행렬 W, 타겟 y, 손실함수 Loss가 있을 때,\n",
    "    - 입력 데이터(x)와 타겟(y)이 고정되어있다면 **loss 함수는 w를 손실 값에 매핑하는 함수**라 할 수 있음\n",
    "        ```py\n",
    "        y_pred = dot(W, x)\n",
    "        loss_value = loss(y_pred, y)\n",
    "        ```\n",
    "    - 다시 말해, 특정 W에 대한 Loss의 **Gradient(W이 Loss에 얼마나 기여하는지의 정도)**를 구해서 W을 업데이트 할 수 있다.\n",
    "        - **W_new = W_old - Learning_rate * Gradient(Loss)(W_old)**\n",
    "        - W는 tensor\n",
    "        - Gradient(Loss)(W_old)[i, j] 는 W_old의 i, j를 변경했을 때 Loss가 바뀌는 정도(방향과 크기)\n",
    "        - 즉 Tensor인 **Gradient(Loss)(W_old)**는 **W_old에 대한 Loss(W_old)의 Gradient**이다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "mini-batch stochastic gradient\n",
    "1. 랜덤한 훈련 샘플 데이터 x, y 추출\n",
    "2. x로 네트워크를 실행, 예측 y_pred 구하기\n",
    "3. y_pred와 y 사이의 오차 측정 Loss 계산\n",
    "4. 네트워크의 가중치에 대한 Loss 함수의 그래디언트 계산(backward pass)\n",
    "5. 그래디언트 반대 방향으로 가중치 업데이트 (W -= step * gradient)\n",
    "\n",
    "step값을 적절히 골라야 local minimum에 갖히지 않는다고 함(너무 작으면...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고차원 공간\n",
    "- 뉴럴넷 알고리즘이 로컬 미니멈에 쉽게 갇힐 것 같지만,\n",
    "- 고차원 공간에서는 대부분 saddle point로 나타나고, 로컬 미니멈은 매우 드물다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD의 변종\n",
    "- Momentum 이용\n",
    "    - 모멘텀 ; SGD의 2가지 문제점(수렴 속도, 로컬 미니멈) 해결\n",
    "        - 어떤 네트워크의 파라미터 하나에 대한 손실 값의 곡선이 있을 때,\n",
    "        - 파라미터 값에 따라 로컬 미니멈에 도달 할 수 있고, 왼쪽으로 가든 오른쪽으로 가든 손실이 증가하는 상황\n",
    "        - 하지만 골짜기를 넘어 섰을 때 글로벌 미니멈이 존재할 때, 작은 학습률을 가진 SGD라면 로컬 미니멈을 벗어날 수 없다\n",
    "            - 이 때, 모멘텀(관성)을 이용해서 이를 해결 할 수 있다.\n",
    "            - 현재 기울기 값뿐만 아니라, 과거로 부터 누적된 기울기 값을 어느정도 고려해서 현재 가중치의 업데이트 정도를 판단한다.\n",
    "        - 예시 구현\n",
    "        ```py\n",
    "        past_velocity = 0.\n",
    "        momentum = 0.1\n",
    "        while loss > 0.01:\n",
    "            w, loss, gradient = get_current_params()\n",
    "            velocity = momentum * past_velocity - learning_rate * gradient\n",
    "            w = w + momentum * velocity - learning_rate * gradient\n",
    "            past_velocity = velocity\n",
    "            update_params(w)\n",
    "        ```\n",
    "- RMSProp, Adagrad..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
