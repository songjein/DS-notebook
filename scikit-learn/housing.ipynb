{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Step\n",
    "1. Look at the big picture\n",
    "2. Get the data\n",
    "3. Discover and visualize the data to gain insights\n",
    "4. Prepare the data for Machine Learning Algorithm\n",
    "5. Select a model and train it\n",
    "6. Fine-tune your model\n",
    "7. Present you solution\n",
    "8. Launch, monitor, and maintain your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Things\n",
    "- the business objective\n",
    "- the current solution\n",
    "- problem definition\n",
    "- check the assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure generally preferred in regression tasks\n",
    "- Root Mean Squared Error (RMSE)\n",
    "    - how much error the system typically makes in its prediction  \n",
    "    with a higher weight for large errors\n",
    "    - Euclidian norm(distance), also called L2 norm\n",
    "- Mean Absolute Error (MAE)\n",
    "    - If there are many outliers, you may consider using this\n",
    "    - L1 norm, Manhattan norm\n",
    "- In L'k' norm, the higher the norm index k, the more it focuses on large values  \n",
    "and neglects small ones\n",
    "- L0 norm just gives the cardinality of the vector\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a Quick Look at the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the numerical attributes\n",
    "- null values are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a few things in these histogram\n",
    "- median income is not expressed in USD\n",
    "- some data has been scaled and capped\n",
    "- capped data can be a serious problem\n",
    "    - prices never go beyond the limit(capped value)\n",
    "    - solution?\n",
    "        - collect the actual value\n",
    "        - remove those data from training & test set\n",
    "- attributes have very different scales\n",
    "- tail heavy!\n",
    "    - transform them to bell-shaped dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Test Set\n",
    "- no data **snooping bias**!\n",
    "    - our brain is amazing pattern detection system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### purely random sampling ?\n",
    "- this is generally fine if our dataset is **large enough**\n",
    "    - but if it is not, can be lead to significant **sampling bias**\n",
    "- if US population is composed of 51.3% female, 48.7% male\n",
    "    - should maintain this ratio in the sample: 513 female & 487 male\n",
    "- **Stratified Sampling**\n",
    "- if the median imcome is important attribute to predict median housing prices\n",
    "    - then the test set is representative of the various categories of incomes in the whole datasets\n",
    "    - so, first we need to divide data into some stratums\n",
    "    - and each stratum have a sufficient number of instances\n",
    "        - or else the estimate of the stratum's importance may be biased\n",
    "- we do not have too many strata, and each stratum should be large enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['income_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['income_cat'].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop([\"income_cat\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and Visualize the Data to Gain Insights\n",
    "- we only exploring the training set\n",
    "- if it is very large, then we may want to sample an **exploration set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "            s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "            c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "            sharex=False)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This image tells...\n",
    "- housing prices are very much related to the location (close to the ocean)\n",
    "- and to the population density\n",
    "- can add new features that measure the proximity to the (main) cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for Correlations\n",
    "- dataset is not too large\n",
    "    - we can compute the **standard correlation coefficient**\n",
    "    - -1 ~ 1\n",
    "        - close to 1 ; strong positive correlation\n",
    "        - close to -1 ; strong negative correlation\n",
    "        - close to 0 ; no linear correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation coefficient\n",
    "- it only measures linear corrleations\n",
    "    - nothing to do with the slope\n",
    "- completely miss out on nonlinear relationships\n",
    "<img src=\"./imgs/cor_ex.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the most promising attr is the median income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### there are some horizental straight lines caused by capping\n",
    "- we may want to try removing the corresponding districts\n",
    "- need to clean them up !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Attributes Combination!!\n",
    "- **interative process**!!\n",
    "- number of rooms per household\n",
    "- the total number of bedrooms **by itself is not very useful**\n",
    "    - compare it to the number of rooms\n",
    "- population per household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedroom_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- Most ML algorithm cannot work with missing features\n",
    "    - Get rid of the row\n",
    "    - Get rid of the col(attribute)\n",
    "    - Set the values to some value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = housing[\"total_bedrooms\"].median()\n",
    "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer\n",
    "- handy class to take care of missing values !\n",
    "- Scikit-learn Design Pattern (p.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "# alternatively: housing_num = housing.select_dtypes(include=[np.number])\n",
    "\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num) # this is ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                         index=list(housing.index.values)) # keep the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[sample_incomplete_rows.index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### put it back into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
    "housing_tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes\n",
    "- Most ML algorithms prefer to work with numbers anyways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem\n",
    "- two nearby values are more similar than two distant values\n",
    "- need **one-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot # sparse matrix (call toarray() to get dense ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformer\n",
    "- fit(), transform(), fit_transform()\n",
    "- TransformerMixin, BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = [\n",
    "    list(housing.columns).index(col)\n",
    "    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n",
    "\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "attr_addr = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_addr.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_extra_attribs = pd.DataFrame(\n",
    "    housing_extra_attribs, \n",
    "    columns=list(housing.columns) + [\"rooms_per_household\", \"population_per_household\"])\n",
    "housing_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, we can use **FunctionTransformer** class\n",
    "- let us easily create a transformer based on a transformation function\n",
    "- validate=False, because the data contains non-float values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def add_extra_features(X, add_bedrooms_per_room=True):\n",
    "    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "    population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "    if add_bedrooms_per_room:\n",
    "        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "        return np.c_[X, rooms_per_household, population_per_household,\n",
    "                     bedrooms_per_room]\n",
    "    else:\n",
    "        return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = FunctionTransformer(add_extra_features, validate=False,\n",
    "                                 kw_args={\"add_bedrooms_per_room\": False})\n",
    "housing_extra_attribs = attr_adder.fit_transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_extra_attribs = pd.DataFrame(\n",
    "    housing_extra_attribs,\n",
    "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"])\n",
    "housing_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling!\n",
    "- ML algorithms don't perform well when the input numerical attrs have very different scales\n",
    "- But, scaling the target values is generally not required\n",
    "- Two common ways\n",
    "    - min-max scaling\n",
    "        - 0-1 range\n",
    "        - (value - min)/max\n",
    "        - more affected by outlier\n",
    "        - **MinMaxScaler**\n",
    "    - standardization\n",
    "        - (value - mean)/variance\n",
    "        - the result dist has unit variance\n",
    "        - does not have specific range -> may be problem for some algorithms\n",
    "            - ex) neural network often expect an input value 0 - 1\n",
    "        - less affected by outlier\n",
    "        - **StandardScaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipelines\n",
    "- Scikit-learn provides the Pipeline class \n",
    "    - to make the transformation steps executed in the right order\n",
    "- **Pipeline** cunstructor takes a list of name/estimator pairs defining a sequence of steps\n",
    "    - All but the last estimator must be transformers\n",
    "- When we call the pipeline's **fit()** method\n",
    "    - it calls **fit_transform()** sequentially on all transformers\n",
    "    - passing the output of each call as the parameter to the next call\n",
    "    - final estimator just calls the **fit()** method\n",
    "- And, **expose the same methods as the final estimator**\n",
    "    - example below shows that it can call fit_transform() because the last estimator(StandardScaler) is transformer, and it has fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    #('attribs_adder', CombinedAttributesAdder()), # preferable option\n",
    "    ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Numerical & Categorical transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared[0], housing_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the full preprocessing pipeline on a few training instances\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### median_housing_values range \\$120000 ~ \\$265000\n",
    "- not satisfying result\n",
    "- example of underfitting the training data\n",
    "    - features may not provide enough info to make good predictions\n",
    "    - or, the model is not powerful enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a DecisionTreeRegressor\n",
    "- powerful model\n",
    "- capable of finding complex nonlinear relationships in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### badly ovefit the data\n",
    "- we don't want to touch the test set until we are ready to lunch a model \n",
    "- so, we need to use part of the training set for **training**, and part for **model validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Evaluation Using Cross-Validation\n",
    "- K-fold cross-validation\n",
    "    - randomly splits the training set into 10 district subsets called **fold**\n",
    "    - then trains & evaluates the model 10 times, picking up a different fold for evaluation\n",
    "- scikit-learn cross-validation features expect a **utility function(greater is better)**\n",
    "rather than a **cost function(lower is better)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "- training **many Decision Trees** on **random subsets of the features**,\n",
    "    - then averaging out their predictions\n",
    "- building a model on top of many other models is called **Ensemble Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "pd.Series(np.sqrt(-scores)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still Overfitting !!\n",
    "- the score on the training set is still much lower than on the validation sets\n",
    "- Solutions\n",
    "    - simplify the model\n",
    "    - constrain it (regularize it)\n",
    "    - get a lot more training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Goal of these series of works is to shortlist a few promising models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Your Model\n",
    "- if we got a shortlist of promising models\n",
    "    - we now need to fine-tune them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "- will evaluate all the possible combinations of hyperparameter values, using cross-validation\n",
    "- when we have no idea what values to try\n",
    "    - try out consecutive powers of 10\n",
    "- refit=True (defualt, when the best estimator is found, retrain it on the whole data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(grid_search.cv_results_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some of the data preparation steps also can be treated as hyperparams!\n",
    "- add feature or not\n",
    "    - using add_bedrooms_per_room hyperparamter of our CombinedAttributesAdder transformer\n",
    "- find the best way to handle outliers, missing features, feature selection, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search \n",
    "- if the hyperparams **search space is large**\n",
    "- often prefereable to use **RandomizedSearchCV** instead\n",
    "    - evaluates a given number(iteration) of **random combinations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Methods\n",
    "- The group will often perform better than the individual Decision Trees,\n",
    "- especially **if the individual models makes very different types of errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With this info\n",
    "- we may want to try **dropping some of the less useful features**\n",
    "    - e.g. only one ocean_proximity category is really useful, could try dropping other categories\n",
    "- And we should also look at the specific errors that our system makes\n",
    "    - then, try to understand why it makes them and how could fix the problem\n",
    "    - **adding extra features or, getting rid of uninformatives ones, cleaning up outliers...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Your System on the Test Set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resist the Temptation!\n",
    "- The performance usually be slightly worse than what we measured using CV if we did a lot of hyperparams tuning\n",
    "    - because our system ends up fine-tuned to perform well on the validation set\n",
    "    - and will not perform as well on unknown data\n",
    "- Must resist the temptation to tweak the hyperparams to make the numbers look good on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
